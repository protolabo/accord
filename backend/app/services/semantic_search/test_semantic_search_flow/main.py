"""
Point d'entr√©e FastAPI pour le service de recherche s√©mantique Accord.
Configure l'application avec tous les endpoints et middlewares n√©cessaires.

üéØ R√©sultat

Cas simple : spaCy seul (rapide)
Cas complexe : spaCy + LLM fusionn√©s (qualit√©)

"""
import time
import logging
from contextlib import asynccontextmanager

import psutil
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

debuguerBreakpoint = True
from backend.app.services.semantic_search.endpoints import router
from backend.app.services.semantic_search.llm_engine import get_query_parser
from backend.app.services.semantic_search.query_transformer import get_query_transformer

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

debuguerBreakpoint = True
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestionnaire de cycle de vie de l'application"""

    # Startup
    logger.info("üöÄ D√©marrage du service de recherche s√©mantique Accord")

    # Pr√©-chargement des mod√®les pour optimiser les premi√®res requ√™tes
    logger.info("üì• Pr√©-chargement des composants...")

    try:
        # Initialiser le parser LLM
        debuguerBreakpoint = True
        llm_parser = get_query_parser()
        if llm_parser.model:
            logger.info("‚úÖ Mod√®le LLM Mistral 7B charg√©")
        else:
            logger.warning("‚ö†Ô∏è Mod√®le LLM non disponible, fallback activ√©")

        # Initialiser le transformer
        transformer = get_query_transformer()
        logger.info("‚úÖ Query transformer initialis√©")

        # Test de sanit√© au demarrage
        """
            - V√©rifier que le pipeline complet fonctionne (LLM + transformer)
            - Mesurer les performances au d√©marrage
            - S'assurer que tous les composants sont correctement charg√©s
        """
        test_query = "emails de test"
        start_time = time.time()
        from backend.app.services.semantic_search.models import NaturalLanguageRequest
        # cr√©e un objet Pydantic pour encapsuler la requ√™te,
        test_request = NaturalLanguageRequest(query=test_query)
        debuguerBreakpoint = True
        result = transformer.transform_query(test_request) #Go to : query_transformer.transform_query
        print(result)


        test_time = (time.time() - start_time) * 1000
        logger.info(f"‚úÖ Test de sanit√© r√©ussi en {test_time:.1f}ms")

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'initialisation: {e}")
    logger.info("üéØ Service de recherche s√©mantique pr√™t")

    yield

    # Shutdown
    logger.info("üõë Arr√™t du service de recherche s√©mantique")


# Cr√©ation de l'application FastAPI
app = FastAPI(
    title="Accord Semantic Search Service",
    description="Service de recherche s√©mantique pour l'application Accord - Transforme le langage naturel en requ√™tes structur√©es",
    version="1.0.0",
    lifespan=lifespan
)

# Configuration CORS pour d√©veloppement
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # React development
        "http://localhost:8080",
        "http://127.0.0.1:3000",
        "http://127.0.0.1:8080",
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)


# Middleware de logging des requ√™tes
@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Log toutes les requ√™tes avec timing"""
    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss / 1024 ** 2

    # Log de la requ√™te entrante
    logger.info(f"üì• {request.method} {request.url.path}")

    response = await call_next(request)

    end_time = time.time()
    end_memory = psutil.Process().memory_info().rss / 1024 ** 2

    # Log de la r√©ponse avec timing
    process_time = (time.time() - start_time) * 1000
    logger.info(f"üì§ {request.method} {request.url.path} - {response.status_code} ({process_time:.1f}ms)")

    # Logs pour monitoring
    logger.info(f"üîç {request.url.path} - "
                f"Latency: {(end_time - start_time) * 1000:.1f}ms - "
                f"Memory: {end_memory:.1f}MB (+{end_memory - start_memory:.1f}MB)")

    response.headers["X-Process-Time"] = str(process_time)

    return response


# Gestionnaire d'erreurs global
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Gestionnaire d'erreurs global pour debugging"""
    logger.error(f"‚ùå Erreur non g√©r√©e sur {request.url.path}: {str(exc)}", exc_info=True)

    return JSONResponse(
        status_code=500,
        content={
            "error": "Erreur interne du serveur",
            "detail": str(exc) if app.debug else "Une erreur inattendue s'est produite",
            "path": str(request.url.path),
            "timestamp": time.time()
        }
    )


# Inclusion du router principal
app.include_router(router)




# Point d'entr√©e pour uvicorn
if __name__ == "__main__":
    import uvicorn

    debuguerBreakpoint = True
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )